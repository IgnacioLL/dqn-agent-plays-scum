Timer unit: 1e-06 s

Total time: 864.768 s
File: /mnt/c/Users/igllo/projects/dqn-agent-plays-scum/dqn_agent.py
Function: train at line 68

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    68                                               @profile
    69                                               def train(self, terminal_state):
    70     37703    5813740.2    154.2      0.7          self.model.train()
    71                                           
    72                                                   ## Initialize the running loss to 0 this will be used to store the loss of the training
    73     37703      20422.1      0.5      0.0          running_loss = 0.0
    74                                           
    75                                                   # Start training only if certain number of samples is already saved
    76     37703      73062.0      1.9      0.0          if len(self.replay_memory) < C.MIN_REPLAY_MEMORY_SIZE:
    77     23737      11073.7      0.5      0.0              return
    78                                                       
    79                                                   ## Sample a minibatch from the replay memory
    80     13966   54136177.3   3876.3      6.3          minibatch = random.sample(self.replay_memory, C.BATCH_SIZE)
    81                                           
    82                                                   ## Predict the Q's value for the current states
    83  14315150   16898986.2      1.2      2.0          current_states = torch.stack([transition[0] for transition in minibatch]).to(C.DEVICE) 
    84     13966   85456682.8   6118.9      9.9          current_qs_list = self.predict(current_states, target=False)
    85                                           
    86                                                   ## Predict the Q's value for the new current states with the target model, which is the one that is updated every C.UPDATE_TARGET_EVERY episodes
    87  14315150   18712958.6      1.3      2.2          new_current_states = torch.stack([transition[3] for transition in minibatch]).to(C.DEVICE)
    88     13966   76687427.6   5491.0      8.9          future_qs_list = self.predict(new_current_states, target=True)
    89                                           
    90     13966       9414.6      0.7      0.0          X = []
    91     13966      12524.6      0.9      0.0          y = []
    92                                           
    93                                                   # Now we need to enumerate our batches
    94  14315150   10695368.1      0.7      1.2          for index, (current_state, action, reward, new_current_state, finish) in enumerate(minibatch):
    95                                           
    96                                                       # If not a terminal state, get new q from future states, otherwise set it to 0
    97                                                       # almost like with Q Learning, but we use just part of equation here
    98  14301184    3944232.5      0.3      0.5              if not finish:
    99                                                           # that .item() does is to get the value of the tensor as a python number    
   100  13581477  107128764.6      7.9     12.4                  max_future_q = torch.max(future_qs_list[index]).item() 
   101  13581477    4845542.2      0.4      0.6                  new_q = reward + C.DISCOUNT * max_future_q
   102                                                       else:
   103    719707     151026.5      0.2      0.0                  new_q = reward
   104                                           
   105                                                       # Update Q value for given state
   106                                                       ## We clone the tensor to avoid in place operations
   107  14301184   94156949.0      6.6     10.9              current_qs = current_qs_list[index].clone()
   108  14301184   81103497.4      5.7      9.4              current_qs[action-1] = new_q
   109                                           
   110                                                       # And append to our training data
   111  14301184   13861230.8      1.0      1.6              X.append(current_state)
   112  14301184   10061455.1      0.7      1.2              y.append(current_qs)
   113                                                   
   114                                                   ## Convert the lists to tensors
   115     13966    8287815.7    593.4      1.0          batch_X = torch.stack(X).to(C.DEVICE)
   116     13966    4783436.4    342.5      0.6          batch_y = torch.stack(y).to(C.DEVICE)
   117                                           
   118     27932   13436465.0    481.0      1.6          with amp.autocast():
   119     13966   74115407.8   5306.8      8.6              outputs = self.model(batch_X)
   120     13966    4298321.4    307.8      0.5              loss = self.criterion(outputs, batch_y)
   121                                           
   122                                                       # Backward pass and optimize
   123                                                       ## this is for the mixed precision training,
   124                                                       # it is used to scale the loss and the gradients 
   125                                                       # so that the training is more stable
   126     13966  140987609.0  10095.1     16.3              self.scaler.scale(loss).backward() 
   127     13966   34879089.3   2497.4      4.0              self.scaler.step(self.optimizer)
   128                                                       # Update the scale for next iteration
   129     13966      60645.4      4.3      0.0              self.scaler.update()
   130                                                       
   131     13966      73340.4      5.3      0.0              running_loss += loss.item()
   132                                                   
   133                                           
   134                                                   # Update target network counter every episode
   135     13966       6826.7      0.5      0.0          if terminal_state:
   136       742        477.9      0.6      0.0              self.target_update_counter += 1
   137                                           
   138                                                   # If counter reaches set value, update target network with weights of main network
   139     13966      57731.3      4.1      0.0          if self.target_update_counter > C.UPDATE_TARGET_EVERY:
   140                                                       self.target_model.load_state_dict(self.model.state_dict())
   141                                                       self.target_update_counter = 0

Total time: 7.5773 s
File: /mnt/c/Users/igllo/projects/dqn-agent-plays-scum/env.py
Function: get_cards_to_play at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                               @profile
   152                                               def get_cards_to_play(self) -> torch.tensor:
   153     39253     156382.9      4.0      2.1          if sum(self.players_in_game) == 0:
   154                                                       return
   155                                           
   156     39253      30193.6      0.8      0.4          if self.last_player == self.player_turn:
   157      1550      14242.8      9.2      0.2              self._reinitialize_round()
   158      1550       4283.9      2.8      0.1              return self.get_cards_to_play()
   159                                           
   160     37703    7344745.2    194.8     96.9          action_state: torch.tensor = self._get_cards_to_play()
   161                                                   
   162     37703      27454.0      0.7      0.4          return action_state

Total time: 13.9628 s
File: /mnt/c/Users/igllo/projects/dqn-agent-plays-scum/env.py
Function: decide_move at line 165

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   165                                               @profile
   166                                               def decide_move(self, action_state: torch.tensor, epsilon: float=1, agent: torch.nn.Module=None) -> int:
   167     37703      23948.1      0.6      0.2          if action_state is None:
   168                                                       action_state = torch.tensor([0 for _ in range((C.NUMBER_OF_CARDS_PER_SUIT+1)*C.NUMBER_OF_SUITS)] + [1], dtype=torch.float32).to(C.DEVICE)
   169     37703     144864.6      3.8      1.0          if random.random() < epsilon:
   170     36440     736410.9     20.2      5.3              action_state_list = action_state.cpu().detach().numpy()
   171   2113520    6802633.7      3.2     48.7              indices = [i for i, x in enumerate(action_state_list) if x == 1]
   172     36440     464767.1     12.8      3.3              return random.choice(indices) + 1
   173                                                   else:
   174      1263    2045659.4   1619.7     14.7              prediction = agent.predict(action_state, target=True)
   175      1263     119886.9     94.9      0.9              print("Using model to decide move")
   176                                           
   177      1263     100845.6     79.8      0.7              print("Prediction shape is: ", prediction.shape)
   178      1263      38352.2     30.4      0.3              print("Action state shape is: ", action_state.shape)
   179                                           
   180      1263     961304.2    761.1      6.9              print("For the action state: ", action_state[0])
   181      1263    2330622.3   1845.3     16.7              print("Prediction made by the model is: ", prediction[0].cpu().detach().numpy().round(2))
   182                                                       
   183                                                       # We set a large negative value to the masked predictions that are not possible
   184      1263      68780.9     54.5      0.5              masked_predictions = prediction[0] * action_state
   185      1263      32767.2     25.9      0.2              masked_predictions_npy  = masked_predictions.cpu().detach().numpy()
   186                                           
   187      1263      31458.0     24.9      0.2              masked_predictions_npy[masked_predictions_npy == 0] = -1_000
   188                                           
   189                                                       # if int(random.random()*1000) % 1000 == 0:
   190                                                       #     self._print_model_prediction(prediction, masked_predictions_npy)
   191                                                       
   192      1263      60526.6     47.9      0.4              return np.argsort(masked_predictions_npy)[-1] + 1  ## esto devolvera un valor entre 1 y 57 que sera la eleccion del modelo

Total time: 8.41178 s
File: /mnt/c/Users/igllo/projects/dqn-agent-plays-scum/env.py
Function: make_move at line 200

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   200                                               @profile
   201                                               def make_move(self, action: int) -> Tuple[torch.tensor, int, bool, int]:
   202                                                   ## This will be the variables returned to compute the reward.
   203                                                   ## We will use the past events to be able to compute the new states.
   204     37703      34840.3      0.9      0.4          agent_number = self.player_turn
   205                                           
   206     37703      32249.0      0.9      0.4          previous_state = self.previous_state[agent_number]
   207     37703      19805.5      0.5      0.2          previous_reward = self.previous_reward[agent_number]
   208     37703    5936236.5    157.4     70.6          new_state = self.convert_to_binary_player_turn_cards() ## this is the new state
   209                                           
   210                                                   # Update previous state to the new state
   211     37703      21420.3      0.6      0.3          self.previous_state[agent_number] = new_state
   212                                           
   213     37703     149959.6      4.0      1.8          n_cards, card_number = self._decode_action(action)
   214                                           
   215     37703      75078.4      2.0      0.9          if self._is_pass_action(n_cards):
   216     18708     312259.1     16.7      3.7              return self._handle_pass_action(previous_state, new_state, previous_reward, False, agent_number)
   217                                           
   218     18995      43970.6      2.3      0.5          skip = self._is_skip_move(card_number)
   219                                           
   220                                                   ## Delete the cards played from the player's hand also last move and last player to play.
   221     18995    1419513.0     74.7     16.9          self._update_game_state(card_number, n_cards)
   222                                                   
   223                                                   ## Check if the player has finished the game and compute the reward
   224     18995      90110.9      4.7      1.1          finish, finishing_reward = self._check_player_finish()
   225     18995      70376.7      3.7      0.8          cards_reward = self._calculate_cards_reward(n_cards, finish)
   226     18995      14698.2      0.8      0.2          total_reward = cards_reward + finishing_reward
   227                                           
   228                                                   ## Update the self.previous_reward and self.previous_finish
   229     18995      48026.5      2.5      0.6          self._update_previous_state(total_reward, finish)
   230                                           
   231                                                   ## This changes self.player_turn
   232     18995     112685.1      5.9      1.3          self._update_player_turn(skip)
   233                                           
   234     18995      30545.4      1.6      0.4          return previous_state, new_state, previous_reward, finish, agent_number

Total time: 0.0958134 s
File: /mnt/c/Users/igllo/projects/dqn-agent-plays-scum/env.py
Function: get_stats_after_finish at line 236

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   236                                               @profile
   237                                               def get_stats_after_finish(self, agent_number: int) -> Tuple[int, int, int]:
   238      2000       1007.0      0.5      1.1          current_state = self.previous_state[agent_number]
   239      2000      92572.6     46.3     96.6          new_state = torch.zeros((C.NUMBER_OF_CARDS_PER_SUIT+1)*C.NUMBER_OF_SUITS+1)
   240      2000        688.6      0.3      0.7          reward = self.previous_reward[agent_number]
   241      2000       1545.3      0.8      1.6          return current_state, new_state, reward

Total time: 162.807 s
File: /root/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py
Function: decorate_context at line 112

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   112                                               @functools.wraps(func)
   113                                               def decorate_context(*args, **kwargs):
   114     58390    2150484.2     36.8      1.3          with ctx_factory():
   115     29195  160656828.5   5502.9     98.7              return func(*args, **kwargs)

